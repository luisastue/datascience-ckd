{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Comparing the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_calcul(metrics):\n",
    "    mean_metric = []\n",
    "    for metric in metrics.columns:\n",
    "        mean = metrics[metric].mean()\n",
    "        mean_metric.append(mean)\n",
    "    return mean_metric\n",
    "\n",
    "def mean_metrics(m):\n",
    "    m1 = metric_calcul(m)\n",
    "    m2 = []\n",
    "    for imp_mean in m1:\n",
    "        m2.append(np.mean(imp_mean))\n",
    "    return m2\n",
    "\n",
    "def metrics_printer(m):\n",
    "    names = ['Accuracy', 'Precision', 'Recall', 'f1_score']\n",
    "    for i, metric in enumerate(mean_metrics(m)):\n",
    "        print(f'Mean of Performance metric {names[i]}:\\t {metric}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> KNN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Performance metric Accuracy:\t 0.8185584946454512\n",
      "Mean of Performance metric Precision:\t 0.8272482427722729\n",
      "Mean of Performance metric Recall:\t 0.8185584946454512\n",
      "Mean of Performance metric f1_score:\t 0.8193853139744962\n"
     ]
    }
   ],
   "source": [
    "metrics_knn = pd.read_csv('metrics_knn.csv')\n",
    "\n",
    "metrics_printer(metrics_knn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Decision Tree Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Performance metric Accuracy:\t 0.9718814398162224\n",
      "Mean of Performance metric Precision:\t 0.97436011970909\n",
      "Mean of Performance metric Recall:\t 0.9718814398162224\n",
      "Mean of Performance metric f1_score:\t 0.9719535753523486\n"
     ]
    }
   ],
   "source": [
    "metrics_dtree = pd.read_csv('metrics_dtree.csv')\n",
    "\n",
    "metrics_printer(metrics_dtree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Performance metric Accuracy:\t 0.9870933414411674\n",
      "Mean of Performance metric Precision:\t 0.9881731473035821\n",
      "Mean of Performance metric Recall:\t 0.9870933414411674\n",
      "Mean of Performance metric f1_score:\t 0.987204241726609\n"
     ]
    }
   ],
   "source": [
    "metrics_logreg = pd.read_csv('metrics_logreg.csv')\n",
    "\n",
    "metrics_printer(metrics_logreg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forests Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Performance metric Accuracy:\t 1.0\n",
      "Mean of Performance metric Precision:\t 1.0\n",
      "Mean of Performance metric Recall:\t 1.0\n",
      "Mean of Performance metric f1_score:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "metrics_rf = pd.read_csv('metrics_rf.csv')\n",
    "\n",
    "metrics_printer(metrics_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SVM Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of Performance metric Accuracy:\t 0.9841073781291173\n",
      "Mean of Performance metric Precision:\t 0.9853379420498986\n",
      "Mean of Performance metric Recall:\t 0.9841073781291173\n",
      "Mean of Performance metric f1_score:\t 0.984173307758821\n"
     ]
    }
   ],
   "source": [
    "metrics_svm = pd.read_csv('metrics_svm.csv')\n",
    "\n",
    "metrics_printer(metrics_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Precision\n",
      "Recall\n",
      "f1_score\n",
      "Accuracy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[262], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m         dicto[key]\u001b[39m.\u001b[39mextend(values)\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m dicto          \n\u001b[0;32m---> 38\u001b[0m dictionary \u001b[39m=\u001b[39m dict_concat(dict_dtree, dict_knn, dict_logreg, dict_rf, dict_svm)\n",
      "Cell \u001b[0;32mIn[262], line 27\u001b[0m, in \u001b[0;36mdict_concat\u001b[0;34m(d1, d2, d3, d4, d5)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m key, values \u001b[39min\u001b[39;00m d1\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     26\u001b[0m     \u001b[39mprint\u001b[39m(key)\n\u001b[0;32m---> 27\u001b[0m     dicto[key]\u001b[39m.\u001b[39mextend(values)\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m key, values \u001b[39min\u001b[39;00m d2\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     29\u001b[0m     dicto[key]\u001b[39m.\u001b[39mextend(values)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Accuracy'"
     ]
    }
   ],
   "source": [
    "def dict_maker(metric):\n",
    "        mean = []\n",
    "        keys = []\n",
    "        names = ['Accuracy', 'Precision', 'Recall', 'f1_score']\n",
    "        for i, metric in enumerate(mean_metrics(metric)):\n",
    "            mean.append(metric)\n",
    "            keys.append(names[i])\n",
    "        dictionary = dict(zip(keys, mean))\n",
    "        return dictionary\n",
    "\n",
    "dict_dtree = dict_maker(metrics_dtree)\n",
    "dict_knn = dict_maker(metrics_knn)\n",
    "dict_logreg = dict_maker(metrics_logreg)\n",
    "dict_rf = dict_maker(metrics_rf)\n",
    "dict_svm = dict_maker(metrics_svm)\n",
    "\n",
    "for keys, values in dict_dtree.items():\n",
    "     print(keys)\n",
    "\n",
    "def dict_concat(d1, d2, d3, d4, d5):\n",
    "    dicto = {'Accuaracy': [],\n",
    "             'Precision': [],\n",
    "             'Recall': [],\n",
    "             'f1_score': []}\n",
    "    for key, values in d1.items():\n",
    "        dicto[key].extend(values)\n",
    "    for key, values in d2.items():\n",
    "        dicto[key].extend(values)\n",
    "    for key, values in d3.items():\n",
    "        dicto[key].extend(values)\n",
    "    for key, values in d4.items():\n",
    "        dicto[key].extend(values)\n",
    "    for key, values in d5.items():\n",
    "        dicto[key].extend(values)\n",
    "    return dicto          \n",
    "\n",
    "dictionary = dict_concat(dict_dtree, dict_knn, dict_logreg, dict_rf, dict_svm)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
